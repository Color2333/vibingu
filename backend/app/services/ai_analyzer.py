"""
AI åˆ†æå™¨ - åŸºäºå†å²æ•°æ®ç”Ÿæˆæ·±åº¦æ´å¯Ÿ
"""

import json
import logging
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from collections import defaultdict

from app.config import get_settings
from app.database import SessionLocal
from app.models import LifeStream
from app.services.json_utils import safe_extract_json
from app.services.ai_client import get_ai_client, AIClientError

logger = logging.getLogger(__name__)
settings = get_settings()


class AIAnalyzer:
    """AI é©±åŠ¨çš„æ•°æ®åˆ†æå™¨"""
    
    def __init__(self):
        # ä½¿ç”¨ç»Ÿä¸€çš„ AI å®¢æˆ·ç«¯ï¼ˆå¸¦ Token è¿½è¸ªï¼‰
        try:
            self.ai_client = get_ai_client()
            self.has_ai = self.ai_client.client is not None
        except Exception:
            self.ai_client = None
            self.has_ai = False
    
    def _get_db(self) -> Session:
        return SessionLocal()
    
    async def analyze_weekly_data(self) -> Dict[str, Any]:
        """
        åˆ†æè¿‡å»ä¸€å‘¨çš„æ•°æ®ï¼Œç”Ÿæˆ AI æ´å¯Ÿ
        """
        db = self._get_db()
        try:
            start_date = datetime.now() - timedelta(days=7)
            records = db.query(LifeStream).filter(
                LifeStream.created_at >= start_date
            ).order_by(LifeStream.created_at.desc()).all()
            
            if not records:
                return {
                    "has_data": False,
                    "summary": "æš‚æ— æ•°æ®ï¼Œå¼€å§‹è®°å½•ä½ çš„ç”Ÿæ´»å§ï¼",
                    "insights": [],
                    "suggestions": []
                }
            
            # æ±‡æ€»æ•°æ®
            summary_data = self._summarize_records(records)
            
            if not self.has_ai:
                return self._mock_analysis(summary_data)
            
            # AI åˆ†æ
            return await self._ai_analyze_weekly(summary_data)
        finally:
            db.close()
    
    async def analyze_trends(self, days: int = 30) -> Dict[str, Any]:
        """
        åˆ†æè¶‹åŠ¿ï¼Œæ‰¾å‡ºæ¨¡å¼å’Œå˜åŒ–
        """
        db = self._get_db()
        try:
            start_date = datetime.now() - timedelta(days=days)
            records = db.query(LifeStream).filter(
                LifeStream.created_at >= start_date
            ).order_by(LifeStream.created_at).all()
            
            if len(records) < 7:
                return {
                    "has_data": False,
                    "message": "æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦7æ¡è®°å½•è¿›è¡Œè¶‹åŠ¿åˆ†æ",
                    "trends": []
                }
            
            summary_data = self._summarize_records(records)
            
            if not self.has_ai:
                return self._mock_trend_analysis(summary_data)
            
            return await self._ai_analyze_trends(summary_data, days)
        finally:
            db.close()
    
    async def generate_smart_suggestions(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ™ºèƒ½å»ºè®®
        """
        db = self._get_db()
        try:
            # è·å–æœ€è¿‘çš„æ•°æ®
            start_date = datetime.now() - timedelta(days=14)
            records = db.query(LifeStream).filter(
                LifeStream.created_at >= start_date
            ).order_by(LifeStream.created_at.desc()).all()
            
            if not records:
                return {
                    "suggestions": ["å¼€å§‹è®°å½•ä½ çš„ç”Ÿæ´»ï¼ŒAI å°†ä¸ºä½ æä¾›ä¸ªæ€§åŒ–å»ºè®®"],
                    "focus_area": None
                }
            
            summary_data = self._summarize_records(records)
            
            if not self.has_ai:
                return self._mock_suggestions(summary_data)
            
            return await self._ai_generate_suggestions(summary_data)
        finally:
            db.close()
    
    async def generate_daily_digest(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆä»Šæ—¥ AI ç»¼åˆæ´å¯Ÿï¼ˆåˆå¹¶äº†å¥åº·æé†’ + å¼‚å¸¸æ£€æµ‹ + å»ºè®®ï¼‰
        """
        db = self._get_db()
        try:
            # è·å–ä»Šæ—¥æ•°æ®
            today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            today_records = db.query(LifeStream).filter(
                LifeStream.created_at >= today_start,
                LifeStream.is_deleted == False,
            ).order_by(LifeStream.created_at.desc()).all()

            # è·å–è¿‘ 7 å¤©æ•°æ®åšå¯¹æ¯”
            week_start = datetime.now() - timedelta(days=7)
            week_records = db.query(LifeStream).filter(
                LifeStream.created_at >= week_start,
                LifeStream.is_deleted == False,
            ).order_by(LifeStream.created_at.desc()).all()

            if not today_records and not week_records:
                return {
                    "has_data": False,
                    "status_summary": "è¿˜æ²¡æœ‰è®°å½•ï¼Œå¼€å§‹è®°å½•ä½ çš„ç”Ÿæ´»å§ï¼",
                    "findings": [],
                    "suggestions": [],
                    "encouragement": "æ¯ä¸€æ¬¡è®°å½•éƒ½æ˜¯å¯¹è‡ªå·±çš„å…³æ³¨",
                }

            today_summary = self._summarize_records(today_records) if today_records else {}
            week_summary = self._summarize_records(week_records) if week_records else {}

            # æ”¶é›†ä»Šæ—¥ç»´åº¦åˆ†æ•°
            today_dimensions = []
            for r in today_records:
                if r.dimension_scores and isinstance(r.dimension_scores, dict):
                    today_dimensions.append({
                        "category": r.category,
                        "scores": r.dimension_scores,
                        "insight": (r.ai_insight or "")[:80],
                    })

            if not self.has_ai:
                return self._mock_daily_digest(today_summary, week_summary)

            return await self._ai_daily_digest(today_summary, week_summary, today_dimensions)
        finally:
            db.close()

    async def _ai_daily_digest(
        self,
        today: Dict,
        week: Dict,
        dimensions: List[Dict],
    ) -> Dict[str, Any]:
        """LLM ç”Ÿæˆç»¼åˆæ¯æ—¥æ´å¯Ÿ"""
        prompt = f"""ä½ æ˜¯ Vibing u çš„ç§äººç”Ÿæ´»åˆ†æå¸ˆã€‚è¯·åŸºäºç”¨æˆ·ä»Šæ—¥å’Œæœ¬å‘¨çš„æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„ç»¼åˆæ´å¯ŸæŠ¥å‘Šã€‚

ã€ä»Šæ—¥æ•°æ®ã€‘
- è®°å½•æ•°: {today.get('total_records', 0)}
- åˆ†ç±»: {json.dumps(today.get('categories', {}), ensure_ascii=False)}
- å¿ƒæƒ…: {today.get('moods', [])[:5] or 'æœªè®°å½•'}
- ç¡çœ : {json.dumps(today.get('sleep_data', [])[:2], ensure_ascii=False) or 'æœªè®°å½•'}
- å±å¹•: {json.dumps(today.get('screen_data', [])[:2], ensure_ascii=False) or 'æœªè®°å½•'}
- è¿åŠ¨: {json.dumps(today.get('activity_data', [])[:2], ensure_ascii=False) or 'æœªè®°å½•'}
- ç»´åº¦è¯„åˆ†: {json.dumps(dimensions[:5], ensure_ascii=False) if dimensions else 'æ— '}

ã€è¿‘7å¤©å‚ç…§ã€‘
- æ€»è®°å½•: {week.get('total_records', 0)}
- åˆ†ç±»åˆ†å¸ƒ: {json.dumps(week.get('categories', {}), ensure_ascii=False)}
- æ ‡ç­¾: {list(week.get('tags', {}).keys())[:10]}

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{{
    "status_summary": "ä¸€å¥è¯æ¦‚æ‹¬ä»Šæ—¥æ•´ä½“çŠ¶æ€ï¼ˆ15-30å­—ï¼Œè¦æœ‰æ¸©åº¦ï¼‰",
    "status_emoji": "ä¸€ä¸ªä»£è¡¨ä»Šæ—¥çŠ¶æ€çš„ emoji",
    "findings": [
        {{
            "type": "positive/warning/neutral",
            "icon": "emoji",
            "title": "å‘ç°æ ‡é¢˜ï¼ˆ5-10å­—ï¼‰",
            "detail": "å…·ä½“è¯´æ˜ï¼ˆ20-40å­—ï¼‰ï¼ŒåŸºäºæ•°æ®"
        }}
    ],
    "suggestions": [
        {{
            "icon": "emoji",
            "action": "å…·ä½“å»ºè®®ï¼ˆ10-20å­—ï¼‰",
            "reason": "åŸå› ï¼ˆ10-15å­—ï¼‰"
        }}
    ],
    "encouragement": "ä¸€å¥æ¸©æš–çš„é¼“åŠ±ï¼ˆ15-25å­—ï¼‰"
}}

è¦æ±‚ï¼š
1. findings 2-4 æ¡ï¼Œæ­£é¢/è­¦å‘Š/ä¸­æ€§æ··åˆï¼Œå¿…é¡»åŸºäºå®é™…æ•°æ®
2. suggestions 2-3 æ¡ï¼Œå…·ä½“å¯è¡Œ
3. è¯­æ°”æ¸©æš–ä½†ä¸ç©ºæ´ï¼Œåƒæœ‹å‹ä¸€æ ·
4. å¦‚æœä»Šæ—¥æ•°æ®å°‘ï¼Œå¯ä»¥ç»“åˆæœ¬å‘¨æ•°æ®åˆ†æ"""

        try:
            result = await self.ai_client.chat_completion(
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": "ç”Ÿæˆä»Šæ—¥æ´å¯ŸæŠ¥å‘Šï¼Œåªè¾“å‡ºJSONã€‚"}
                ],
                max_tokens=6000,
                task_type="daily_digest",
                task_description="ä»Šæ—¥ AI æ´å¯Ÿ",
                json_response=True,
            )

            content = result["content"]
            if not content:
                return self._mock_daily_digest(today, week)

            if isinstance(content, dict):
                content["has_data"] = True
                return content

            parsed = safe_extract_json(content, "daily_digest")
            if parsed and isinstance(parsed, dict):
                parsed["has_data"] = True
                return parsed
            return self._mock_daily_digest(today, week)

        except Exception as e:
            logger.error(f"Daily digest ç”Ÿæˆé”™è¯¯: {e}")
            return self._mock_daily_digest(today, week)

    def _mock_daily_digest(self, today: Dict, week: Dict) -> Dict[str, Any]:
        """æ—  AI æ—¶çš„ fallback"""
        total = today.get("total_records", 0)
        cats = today.get("categories", {})
        findings = []

        if cats.get("SLEEP"):
            findings.append({"type": "positive", "icon": "ğŸ˜´", "title": "ç¡çœ å·²è®°å½•", "detail": f"ä»Šå¤©è®°å½•äº† {cats['SLEEP']} æ¡ç¡çœ æ•°æ®"})
        if cats.get("ACTIVITY"):
            findings.append({"type": "positive", "icon": "ğŸƒ", "title": "è¿åŠ¨æ‰“å¡", "detail": f"ä»Šå¤©è¿åŠ¨äº† {cats['ACTIVITY']} æ¬¡"})
        if not cats.get("ACTIVITY") and week.get("categories", {}).get("ACTIVITY", 0) < 2:
            findings.append({"type": "warning", "icon": "âš¡", "title": "è¿åŠ¨ä¸è¶³", "detail": "æœ¬å‘¨è¿åŠ¨æ¬¡æ•°è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æ´»åŠ¨é‡"})

        return {
            "has_data": total > 0 or week.get("total_records", 0) > 0,
            "status_summary": f"ä»Šå¤©å·²è®°å½• {total} æ¡æ•°æ®" if total else "ä»Šå¤©è¿˜æ²¡æœ‰è®°å½•",
            "status_emoji": "ğŸ“Š" if total else "ğŸŒ…",
            "findings": findings or [{"type": "neutral", "icon": "ğŸ“", "title": "å¼€å§‹è®°å½•", "detail": "è®°å½•ç”Ÿæ´»æ•°æ®ï¼Œè§£é” AI æ´å¯Ÿ"}],
            "suggestions": [{"icon": "ğŸ’¡", "action": "è®°å½•ä»Šå¤©çš„ç”Ÿæ´»", "reason": "æ•°æ®è¶Šå¤šåˆ†æè¶Šå‡†ç¡®"}],
            "encouragement": "æ¯ä¸€æ¬¡è®°å½•éƒ½æ˜¯å¯¹è‡ªå·±çš„å…³æ³¨",
        }

    async def deep_insight(self, question: str) -> Dict[str, Any]:
        """
        åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ·±åº¦æ´å¯Ÿ
        """
        db = self._get_db()
        try:
            # è·å–ç›¸å…³æ•°æ®
            start_date = datetime.now() - timedelta(days=30)
            records = db.query(LifeStream).filter(
                LifeStream.created_at >= start_date
            ).order_by(LifeStream.created_at.desc()).limit(100).all()
            
            if not records:
                return {
                    "answer": "æš‚æ— æ•°æ®å¯ä¾›åˆ†æï¼Œè¯·å…ˆè®°å½•ä¸€äº›ç”Ÿæ´»æ•°æ®ã€‚",
                    "confidence": "low"
                }
            
            summary_data = self._summarize_records(records)
            
            if not self.has_ai:
                return {"answer": "AI æœåŠ¡æœªé…ç½®ï¼Œæ— æ³•å›ç­”é—®é¢˜", "confidence": "low"}
            
            return await self._ai_deep_insight(question, summary_data)
        finally:
            db.close()
    
    def _summarize_records(self, records: List[LifeStream]) -> Dict[str, Any]:
        """æ±‡æ€»è®°å½•æ•°æ®"""
        summary = {
            "total_records": len(records),
            "date_range": {
                "start": records[-1].created_at.isoformat() if records else None,
                "end": records[0].created_at.isoformat() if records else None
            },
            "categories": defaultdict(int),
            "daily_counts": defaultdict(int),
            "hourly_distribution": defaultdict(int),
            "moods": [],
            "sleep_data": [],
            "screen_data": [],
            "activity_data": [],
            "diet_data": [],
            "ai_insights": [],
            "tags": defaultdict(int),
        }
        
        for r in records:
            # åˆ†ç±»ç»Ÿè®¡
            if r.category:
                summary["categories"][r.category] += 1
            
            # æ¯æ—¥ç»Ÿè®¡
            if r.created_at:
                day_key = r.created_at.strftime("%Y-%m-%d")
                summary["daily_counts"][day_key] += 1
                summary["hourly_distribution"][r.created_at.hour] += 1
            
            # æ ‡ç­¾ç»Ÿè®¡
            if r.tags:
                for tag in r.tags:
                    summary["tags"][tag] += 1
            
            # AI æ´å¯Ÿæ”¶é›†
            if r.ai_insight:
                summary["ai_insights"].append({
                    "date": r.created_at.isoformat() if r.created_at else None,
                    "category": r.category,
                    "insight": r.ai_insight[:200]
                })
            
            # åˆ†ç±»æ•°æ®æå–
            if r.category == "MOOD" and r.meta_data:
                mood = r.meta_data.get("mood")
                if mood:
                    summary["moods"].append(mood)
            
            if r.category == "SLEEP" and r.meta_data:
                summary["sleep_data"].append({
                    "date": r.created_at.isoformat() if r.created_at else None,
                    "duration": r.meta_data.get("duration_hours"),
                    "quality": r.meta_data.get("quality"),
                    "score": r.meta_data.get("score"),
                })
            
            if r.category == "SCREEN" and r.meta_data:
                top_apps = r.meta_data.get("top_apps") or []
                summary["screen_data"].append({
                    "date": r.created_at.isoformat() if r.created_at else None,
                    "total_time": r.meta_data.get("total_screen_time"),
                    "total_minutes": r.meta_data.get("total_minutes"),
                    "top_apps": top_apps[:3] if top_apps else [],
                    "health_score": r.meta_data.get("health_score"),
                })
            
            if r.category == "ACTIVITY" and r.meta_data:
                summary["activity_data"].append({
                    "date": r.created_at.isoformat() if r.created_at else None,
                    "type": r.meta_data.get("activity_type"),
                    "duration": r.meta_data.get("duration_minutes"),
                    "calories": r.meta_data.get("calories_burned"),
                })
            
            if r.category == "DIET" and r.meta_data:
                food_items = r.meta_data.get("food_items") or []
                summary["diet_data"].append({
                    "date": r.created_at.isoformat() if r.created_at else None,
                    "foods": food_items,
                    "calories": r.meta_data.get("total_calories"),
                    "is_healthy": r.meta_data.get("is_healthy"),
                })
        
        # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
        summary["categories"] = dict(summary["categories"])
        summary["daily_counts"] = dict(summary["daily_counts"])
        summary["hourly_distribution"] = dict(summary["hourly_distribution"])
        summary["tags"] = dict(sorted(summary["tags"].items(), key=lambda x: x[1], reverse=True)[:20])
        
        return summary
    
    async def _ai_analyze_weekly(self, data: Dict) -> Dict[str, Any]:
        """AI å‘¨åº¦åˆ†æ"""
        prompt = f"""ä½ æ˜¯ Vibing u çš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ä»ç”Ÿæ´»è®°å½•ä¸­å‘ç°æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚

ä»¥ä¸‹æ˜¯ç”¨æˆ·è¿‡å»ä¸€å‘¨çš„ç”Ÿæ´»æ•°æ®æ±‡æ€»ï¼š
- æ€»è®°å½•æ•°: {data['total_records']}
- æ—¶é—´èŒƒå›´: {data['date_range']['start']} åˆ° {data['date_range']['end']}
- åˆ†ç±»åˆ†å¸ƒ: {json.dumps(data['categories'], ensure_ascii=False)}
- å¿ƒæƒ…è®°å½•: {data['moods'][:10] if data['moods'] else 'æ— '}
- ç¡çœ æ•°æ®: {json.dumps(data['sleep_data'][:5], ensure_ascii=False) if data['sleep_data'] else 'æ— '}
- å±å¹•æ—¶é—´: {json.dumps(data['screen_data'][:5], ensure_ascii=False) if data['screen_data'] else 'æ— '}
- è¿åŠ¨æ•°æ®: {json.dumps(data['activity_data'][:5], ensure_ascii=False) if data['activity_data'] else 'æ— '}
- é«˜é¢‘æ ‡ç­¾: {json.dumps(list(data['tags'].items())[:10], ensure_ascii=False)}

è¯·ç”Ÿæˆä¸€ä»½æ¸©æš–ã€æœ‰æ´å¯ŸåŠ›çš„å‘¨åº¦åˆ†ææŠ¥å‘Šï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{{
    "summary": "ä¸€å¥è¯æ€»ç»“æœ¬å‘¨çŠ¶æ€ï¼ˆ20-40å­—ï¼‰",
    "highlights": ["äº®ç‚¹1", "äº®ç‚¹2", "äº®ç‚¹3"],
    "concerns": ["éœ€è¦å…³æ³¨çš„é—®é¢˜1", "é—®é¢˜2"],
    "insights": [
        {{"title": "æ´å¯Ÿæ ‡é¢˜", "content": "å…·ä½“æ´å¯Ÿå†…å®¹ï¼ˆ30-50å­—ï¼‰", "emoji": "ç›¸å…³emoji"}},
        {{"title": "æ´å¯Ÿæ ‡é¢˜", "content": "å…·ä½“æ´å¯Ÿå†…å®¹", "emoji": "emoji"}}
    ],
    "suggestions": [
        {{"action": "å…·ä½“å»ºè®®", "reason": "åŸå› ", "priority": "high/medium/low"}}
    ],
    "mood_trend": "up/down/stable",
    "overall_score": 75
}}

æ³¨æ„ï¼š
1. åˆ†æè¦æœ‰æ¸©åº¦ï¼Œåƒæœ‹å‹ä¸€æ ·å…³å¿ƒç”¨æˆ·
2. æ´å¯Ÿè¦å…·ä½“ï¼ŒåŸºäºæ•°æ®è€Œéæ³›æ³›è€Œè°ˆ
3. å»ºè®®è¦å¯è¡Œï¼Œèƒ½ç«‹å³æ‰§è¡Œ"""

        try:
            result = await self.ai_client.chat_completion(
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": "è¯·åˆ†ææˆ‘çš„æ•°æ®ï¼Œåªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"}
                ],
                max_tokens=6000,
                task_type="weekly_analysis",
                task_description="AI å‘¨åº¦åˆ†æ",
                json_response=True,
            )
            
            content = result["content"]
            
            if not content:
                logger.warning("AI è¿”å›ç©ºå†…å®¹")
                return self._mock_analysis(data)
            
            if isinstance(content, dict):
                content["has_data"] = True
                return content
            
            # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
            parsed = safe_extract_json(content, "weekly_analysis")
            if parsed and isinstance(parsed, dict):
                parsed["has_data"] = True
                return parsed
            return self._mock_analysis(data)
                
        except AIClientError as e:
            logger.error(f"AI åˆ†æé”™è¯¯: {e}")
            return self._mock_analysis(data)
        except Exception as e:
            logger.error(f"AI åˆ†æé”™è¯¯: {e}")
            return self._mock_analysis(data)
    
    async def _ai_analyze_trends(self, data: Dict, days: int) -> Dict[str, Any]:
        """AI è¶‹åŠ¿åˆ†æ"""
        prompt = f"""åˆ†æç”¨æˆ·è¿‡å» {days} å¤©çš„ç”Ÿæ´»æ•°æ®è¶‹åŠ¿ã€‚

æ•°æ®æ±‡æ€»ï¼š
- æ€»è®°å½•: {data['total_records']}
- æ¯æ—¥è®°å½•åˆ†å¸ƒ: {json.dumps(data['daily_counts'], ensure_ascii=False)}
- åˆ†ç±»åˆ†å¸ƒ: {json.dumps(data['categories'], ensure_ascii=False)}
- æ—¶æ®µåˆ†å¸ƒ: {json.dumps(data['hourly_distribution'], ensure_ascii=False)}
- ç¡çœ : {len(data['sleep_data'])} æ¡
- è¿åŠ¨: {len(data['activity_data'])} æ¡
- å±å¹•: {len(data['screen_data'])} æ¡

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºè¶‹åŠ¿åˆ†æï¼š
{{
    "overall_trend": "improving/declining/stable",
    "trend_description": "æ•´ä½“è¶‹åŠ¿æè¿°ï¼ˆ30-50å­—ï¼‰",
    "patterns": [
        {{"name": "æ¨¡å¼åç§°", "description": "æè¿°", "impact": "positive/negative/neutral"}}
    ],
    "correlations": [
        {{"factor1": "å› ç´ 1", "factor2": "å› ç´ 2", "relationship": "å…³ç³»æè¿°"}}
    ],
    "predictions": [
        {{"area": "é¢†åŸŸ", "prediction": "é¢„æµ‹å†…å®¹", "confidence": "high/medium/low"}}
    ],
    "action_items": ["å»ºè®®1", "å»ºè®®2"]
}}"""

        try:
            result = await self.ai_client.chat_completion(
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": "åˆ†æè¶‹åŠ¿ï¼Œåªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"}
                ],
                max_tokens=5000,
                task_type="trend_analysis",
                task_description="AI è¶‹åŠ¿åˆ†æ",
                json_response=True,
            )
            
            content = result["content"]
            if not content:
                return self._mock_trend_analysis(data)
            
            if isinstance(content, dict):
                content["has_data"] = True
                content["period_days"] = days
                return content
            
            parsed = safe_extract_json(content, "trend_analysis")
            if parsed and isinstance(parsed, dict):
                parsed["has_data"] = True
                parsed["period_days"] = days
                return parsed
            return self._mock_trend_analysis(data)
                
        except Exception as e:
            logger.error(f"AI è¶‹åŠ¿åˆ†æé”™è¯¯: {e}")
            return self._mock_trend_analysis(data)
    
    async def _ai_generate_suggestions(self, data: Dict) -> Dict[str, Any]:
        """AI ç”Ÿæˆå»ºè®®"""
        prompt = f"""åŸºäºç”¨æˆ·çš„ç”Ÿæ´»æ•°æ®ï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„æ™ºèƒ½å»ºè®®ã€‚

æ•°æ®æ¦‚è§ˆï¼š
- åˆ†ç±»: {json.dumps(data['categories'], ensure_ascii=False)}
- å¿ƒæƒ…: {data['moods'][:5] if data['moods'] else 'æ— '}
- ç¡çœ : {len(data['sleep_data'])} æ¡è®°å½•
- è¿åŠ¨: {len(data['activity_data'])} æ¡è®°å½•
- å±å¹•: {len(data['screen_data'])} æ¡è®°å½•
- æ ‡ç­¾: {list(data['tags'].keys())[:10]}

è¯·ç”Ÿæˆ 3-5 æ¡å…·ä½“ã€å¯æ‰§è¡Œçš„å»ºè®®ï¼ŒJSON æ ¼å¼ï¼š
{{
    "focus_area": "å½“å‰æœ€éœ€è¦å…³æ³¨çš„é¢†åŸŸ",
    "focus_reason": "åŸå› ï¼ˆ20å­—å†…ï¼‰",
    "suggestions": [
        {{
            "title": "å»ºè®®æ ‡é¢˜",
            "description": "å…·ä½“æè¿°å’Œè¡ŒåŠ¨æ­¥éª¤ï¼ˆ30-50å­—ï¼‰",
            "category": "sleep/activity/screen/mood/diet/social",
            "difficulty": "easy/medium/hard",
            "impact": "high/medium/low",
            "emoji": "ç›¸å…³emoji"
        }}
    ],
    "encouragement": "ä¸€å¥é¼“åŠ±çš„è¯"
}}"""

        try:
            result = await self.ai_client.chat_completion(
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": "ç»™æˆ‘ä¸€äº›å»ºè®®ï¼Œåªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"}
                ],
                max_tokens=5000,
                task_type="smart_suggestions",
                task_description="AI æ™ºèƒ½å»ºè®®",
                json_response=True,
            )
            
            content = result["content"]
            if not content:
                return self._mock_suggestions(data)
            
            if isinstance(content, dict):
                return content
            
            parsed = safe_extract_json(content, "suggestions")
            if parsed and isinstance(parsed, dict):
                return parsed
            return self._mock_suggestions(data)
                
        except Exception as e:
            logger.error(f"AI å»ºè®®ç”Ÿæˆé”™è¯¯: {e}")
            return self._mock_suggestions(data)
    
    async def _ai_deep_insight(self, question: str, data: Dict) -> Dict[str, Any]:
        """AI æ·±åº¦æ´å¯Ÿ"""
        prompt = f"""ä½ æ˜¯ç”¨æˆ·çš„ç§äººç”Ÿæ´»æ•°æ®åˆ†æå¸ˆã€‚ç”¨æˆ·é—®äº†ä¸€ä¸ªé—®é¢˜ï¼Œè¯·åŸºäºä»–çš„å†å²æ•°æ®å›ç­”ã€‚

ç”¨æˆ·æ•°æ®ï¼š
- æ€»è®°å½•: {data['total_records']}
- åˆ†ç±»: {json.dumps(data['categories'], ensure_ascii=False)}
- æœ€è¿‘çš„ AI æ´å¯Ÿ: {json.dumps(data['ai_insights'][:5], ensure_ascii=False)}
- æ ‡ç­¾: {list(data['tags'].keys())[:15]}
- å¿ƒæƒ…: {data['moods'][:10] if data['moods'] else 'æ— '}
- ç¡çœ æ•°æ®: {json.dumps(data['sleep_data'][:3], ensure_ascii=False) if data['sleep_data'] else 'æ— '}
- å±å¹•æ•°æ®: {json.dumps(data['screen_data'][:3], ensure_ascii=False) if data['screen_data'] else 'æ— '}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ä»¥ JSON æ ¼å¼å›ç­”ï¼š
{{
    "answer": "è¯¦ç»†å›ç­”ï¼ˆ100-200å­—ï¼‰",
    "confidence": "high/medium/low",
    "data_points": ["æ”¯æŒç»“è®ºçš„æ•°æ®ç‚¹1", "æ•°æ®ç‚¹2"],
    "follow_up_questions": ["å¯èƒ½çš„è¿½é—®1", "è¿½é—®2"]
}}"""

        try:
            result = await self.ai_client.chat_completion(
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": f"{question}\n\nåªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"}
                ],
                model=settings.smart_model,  # æ·±åº¦æ´å¯Ÿä½¿ç”¨é«˜çº§æ¨¡å‹
                max_tokens=6000,
                task_type="deep_insight",
                task_description="AI æ·±åº¦æ´å¯Ÿ",
                json_response=True,
            )
            
            content = result["content"]
            if not content:
                return {"answer": "AI æœªè¿”å›å†…å®¹", "confidence": "low"}
            
            if isinstance(content, dict):
                return content
            
            parsed = safe_extract_json(content, "deep_insight")
            if parsed and isinstance(parsed, dict):
                return parsed
            return {"answer": content, "confidence": "low"}
                
        except Exception as e:
            logger.error(f"AI æ·±åº¦æ´å¯Ÿé”™è¯¯: {e}")
            return {"answer": f"åˆ†æå‡ºé”™: {str(e)}", "confidence": "low"}
    
    def _mock_analysis(self, data: Dict) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿåˆ†æï¼ˆæ—  AI æ—¶ï¼‰"""
        categories = data.get("categories", {})
        total = data.get("total_records", 0)
        
        insights = []
        if categories.get("SLEEP", 0) > 0:
            insights.append({"title": "ç¡çœ è¿½è¸ª", "content": f"æœ¬å‘¨è®°å½•äº† {categories['SLEEP']} æ¬¡ç¡çœ ", "emoji": "ğŸ˜´"})
        if categories.get("ACTIVITY", 0) > 0:
            insights.append({"title": "è¿åŠ¨è®°å½•", "content": f"æœ¬å‘¨è¿åŠ¨ {categories['ACTIVITY']} æ¬¡", "emoji": "ğŸƒ"})
        
        return {
            "has_data": True,
            "summary": f"æœ¬å‘¨å…±è®°å½• {total} æ¡ç”Ÿæ´»æ•°æ®",
            "highlights": ["ä¿æŒäº†è®°å½•ä¹ æƒ¯"],
            "concerns": [],
            "insights": insights,
            "suggestions": [{"action": "ç»§ç»­ä¿æŒè®°å½•ä¹ æƒ¯", "reason": "æ•°æ®è¶Šå¤šåˆ†æè¶Šå‡†ç¡®", "priority": "high"}],
            "mood_trend": "stable",
            "overall_score": 60
        }
    
    def _mock_trend_analysis(self, data: Dict) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿè¶‹åŠ¿åˆ†æ"""
        return {
            "has_data": True,
            "overall_trend": "stable",
            "trend_description": "æ•°æ®é‡è¾ƒå°‘ï¼Œè¶‹åŠ¿åˆ†æéœ€è¦æ›´å¤šæ•°æ®æ”¯æŒ",
            "patterns": [],
            "correlations": [],
            "predictions": [],
            "action_items": ["å¢åŠ æ—¥å¸¸è®°å½•é¢‘ç‡", "è®°å½•æ›´å¤šç±»å‹çš„æ•°æ®"]
        }
    
    def _mock_suggestions(self, data: Dict) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå»ºè®®"""
        categories = data.get("categories", {})
        
        suggestions = []
        if categories.get("SLEEP", 0) < 3:
            suggestions.append({
                "title": "è®°å½•ç¡çœ ",
                "description": "æ¯å¤©è®°å½•ç¡çœ æƒ…å†µï¼Œå¸®åŠ©åˆ†æä½œæ¯è§„å¾‹",
                "category": "sleep",
                "difficulty": "easy",
                "impact": "high",
                "emoji": "ğŸ˜´"
            })
        if categories.get("ACTIVITY", 0) < 2:
            suggestions.append({
                "title": "å¢åŠ è¿åŠ¨",
                "description": "æ¯å‘¨è‡³å°‘è¿åŠ¨3æ¬¡ï¼Œæ¯æ¬¡30åˆ†é’Ÿä»¥ä¸Š",
                "category": "activity",
                "difficulty": "medium",
                "impact": "high",
                "emoji": "ğŸƒ"
            })
        
        if not suggestions:
            suggestions.append({
                "title": "ç»§ç»­ä¿æŒ",
                "description": "ä½ çš„è®°å½•ä¹ æƒ¯å¾ˆå¥½ï¼Œç»§ç»­åšæŒï¼",
                "category": "mood",
                "difficulty": "easy",
                "impact": "medium",
                "emoji": "âœ¨"
            })
        
        return {
            "focus_area": "æ•´ä½“å¥åº·",
            "focus_reason": "å‡è¡¡å‘å±•å„ç»´åº¦",
            "suggestions": suggestions,
            "encouragement": "æ¯ä¸€æ¬¡è®°å½•éƒ½æ˜¯å¯¹è‡ªå·±çš„å…³æ³¨ â¤ï¸"
        }


# å…¨å±€å•ä¾‹
_analyzer: Optional[AIAnalyzer] = None


def get_ai_analyzer() -> AIAnalyzer:
    """è·å– AI åˆ†æå™¨å•ä¾‹"""
    global _analyzer
    if _analyzer is None:
        _analyzer = AIAnalyzer()
    return _analyzer
